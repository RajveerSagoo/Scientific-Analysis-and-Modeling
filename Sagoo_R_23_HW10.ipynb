{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 17 - LINEAR DATA FITTING WITH UNCERTANTIES - HW 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard importing\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data to a numpy array\n",
    "data = np.genfromtxt(\"LinearFit_data.txt\", names=True, dtype=None)\n",
    "\n",
    "# print out all columns we just got for free\n",
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's see what this data looks like\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black')\n",
    "\n",
    "# Cool! The data actually looks pretty linear so we can assume the function we are fitting *is* linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a linear function the normal way\n",
    "def linear(x, m, b):\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a linear fit to the data\n",
    "\n",
    "popt, pcov = opt.curve_fit(linear, data['X'], data['Y'])\n",
    "print(\"raw: \", *popt)\n",
    "print(\"raw: \", *pcov)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black')\n",
    "ax.plot(data['X'],linear(data['X'], *popt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look into wich data are way off the fit.\n",
    "\n",
    "residual = data['Y']-linear(data['X'],*popt)\n",
    "x = np.linspace(5,20,1000)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],residual,s=15,c='black')\n",
    "ax.plot(x,x*0.0,color='purple')\n",
    "ax.set_xlim(6,16)\n",
    "\n",
    "outlier = np.absolute(residual)>0.2\n",
    "ax.scatter(data['X'][outlier],residual[outlier],s=60,edgecolor='cyan',facecolor='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our values for the raw data above:\n",
    "# slope:    -0.058187\n",
    "# intercept: 0.565351\n",
    "popt, pcov = opt.curve_fit(linear, data['X'], data['Y'])\n",
    "print(\"raw: \", *popt)\n",
    "\n",
    "# we can use curve_fit to de-weight outliers automatically\n",
    "\n",
    "# note, result is dependent on choice of loss function\n",
    "# need to see method to trf in order to use least_squares instead of leastsq\n",
    "popt_clean, pcov = opt.curve_fit(linear, data['X'], data['Y'], method='trf', \n",
    "                                 loss='arctan', f_scale=0.20)\n",
    "print(\"outliers accounted for: \", *popt_clean)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black',zorder=2)\n",
    "# cleaner error bars than last time, play around with the parameters\n",
    "#ax.errorbar(data['X'],data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "#            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with our fits\n",
    "ax.plot(data['X'],linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'],linear(data['X'], *popt_clean), label='better')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets account for the errors\n",
    "# by pasing sigma, scipy knows to weight data according to their errors \n",
    "# naturally, sigma is expected to be the same size and correspond to y\n",
    "# notice: this only accounts for y error\n",
    "popt_err, pcov = opt.curve_fit(linear, data['X'], data['Y'], sigma=data['Y_ERR'])\n",
    "print(*popt_err)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['X'],data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['X'],linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'],linear(data['X'], *popt_clean), label='better')\n",
    "ax.plot(data['X'],linear(data['X'], *popt_err), label='errors')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this accurately explore the parameter space though?\n",
    "# And what is the uncertainty on our data?\n",
    "# What if we had x error bars?\n",
    "\n",
    "# we'll explore a method utilizing the common \"monte carlo\" approach\n",
    "def mcFit(x, y, y_err):\n",
    "    slopes = list()\n",
    "    y_ints = list()\n",
    "    iters = 500 \n",
    "    for i in range(iters):\n",
    "        # remember random normal distribution (Gaussian)\n",
    "        weights = np.random.randn(len(y))\n",
    "\n",
    "        y_adj = y + y_err*weights\n",
    "        x_adj = x  \n",
    "\n",
    "        params, other = opt.curve_fit(linear, x_adj, y_adj)\n",
    "        slopes.append(params[0])\n",
    "        y_ints.append(params[1])\n",
    "    \n",
    "    return slopes, y_ints\n",
    "\n",
    "slope, intercept = mcFit(data['X'], data['Y'], data['Y_ERR'])\n",
    "\n",
    "print('mean slope: {:6.4f}, mean intercept: {:6.4f}'.format(np.mean(slope), np.mean(intercept)))\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'], data['Y'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['X'], data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with our fits\n",
    "ax.plot(data['X'], linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_clean), label='better')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_err), label='errors')\n",
    "ax.plot(data['X'], linear(data['X'], np.mean(slope), np.mean(intercept)), label='MC')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh right, that outlier.\n",
    "\n",
    "mask = data['Y'] < 0.3\n",
    "\n",
    "xdat = data['X'][mask]\n",
    "ydat = data['Y'][mask]\n",
    "ydat_err = data['Y_ERR'][mask]\n",
    "\n",
    "print(\"data size: \", len(xdat))\n",
    "\n",
    "slope, intercept = mcFit(xdat, ydat, ydat_err)\n",
    "\n",
    "print('slope: {:+7.4f} $\\pm$ {:6.4f}, mean intercept: {:6.4f}'.format(np.mean(slope), np.std(slope),\n",
    "                                                                          np.mean(intercept)))\n",
    "print(\"\\n\")\n",
    "# lets discuss monte carlo methods a little\n",
    "for i in range(20):\n",
    "    slope, intercept = mcFit(xdat, ydat, ydat_err)\n",
    "    print(\"slope: {:+7.4f} $\\pm$ {:6.4f}, mean intercept: {:6.4f}\".format(np.mean(slope), np.std(slope), \n",
    "                                                                          np.mean(intercept)))\n",
    "\n",
    "# so what we see that our answer changes slightly, but its within the standard deviation\n",
    "# of each set of measurements. So we can quote a reliable answer, with a reliable uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'], data['Y'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['X'], data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "slope, intercept = mcFit(xdat, ydat, ydat_err)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['X'], linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_clean), label='better')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_err), label='errors')\n",
    "ax.plot(data['X'], linear(data['X'], np.mean(slope), np.mean(intercept)), label='MC')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "ax.text(10, 0.4, \"Slope: {:+5.3f} $\\pm$ {:5.3f}\".format(np.mean(slope), np.std(slope)), fontsize=18)\n",
    "ax.text(10, 0.35, \"Intercept: {:5.3f} $\\pm$ {:5.3f}\".format(np.mean(intercept), np.std(intercept)), fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework #10\n",
    "\n",
    "**REMINDER:** *All coding assignment will be turned in as .ipynb files, to the same PHYS_X0223 repository on GitHub.*   \n",
    "*They should be turned in with the following naming:*\n",
    "    \n",
    "    Lastname_Firstinitial_23_HW10.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "We can't know the x data perfectly suppose the data have uniform x errors of 0.5 use the MC technique to estimate the slope *AND* the uncertainty taking into account these errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "Repeat problem #1 , but change the errors INCREASE as we move away from X = 8 (as they do) to be precise: suppose X_ERR = |x - 8| /10, e.g. it increases by .1 per unit.\n",
    "\n",
    "Plot both slopes (Problem 1 & 2). How does the fit change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: \n",
    "Repeat problem 1 & 2, but now anaylze for X vs. Z data, including both uniform 0.5 and X_ERR = |x - 8| /10 uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
